# LangDAG

> LLM Conversations as Directed Acyclic Graphs

LangDAG is a high-performance Go tool for orchestrating LLM interactions. It models conversations as trees of nodes — each message is a node, and prompting from any node extends or branches the tree.

## Core Concept

Everything is a **node**. There is no separate DAG entity. A root node (parent_id = NULL) defines a conversation tree. Prompting from any node creates a child node, enabling natural branching and exploration.

```
┌─────────┐     ┌─────────┐     ┌─────────┐     ┌─────────┐
│  User   │────>│   LLM   │────>│  Tool   │────>│   LLM   │
└─────────┘     └─────────┘     └─────────┘     └─────────┘
                    │                               │
                    └──────── conversation ─────────┘
                                 = node tree
```

## Features

- **One concept**: nodes. A root node defines a conversation tree.
- **One verb**: `Prompt`. Same method on `Client` (new tree) and `Node` (extend tree).
- **Performance**: Pure Go, ~1ms overhead, single static binary, zero runtime deps
- **Native Streaming**: SSE support for real-time token streaming
- **Branching**: Prompt from any node to create alternative paths
- **Persistent Storage**: SQLite (default), full history replay

## Architecture

```
┌──────────────────────────────────────────────────────────┐
│                      CLI / API                           │
├──────────────────────────────────────────────────────────┤
│                   Node Executor                          │
│   ┌───────────┐   ┌───────────┐   ┌───────────┐         │
│   │  Parser   │   │ Scheduler │   │  Runner   │         │
│   └───────────┘   └───────────┘   └───────────┘         │
├──────────────────────────────────────────────────────────┤
│                   Provider Layer                         │
│   ┌───────────┐   ┌───────────┐   ┌───────────┐         │
│   │ Anthropic │   │  OpenAI   │   │  Ollama   │         │
│   └───────────┘   └───────────┘   └───────────┘         │
├──────────────────────────────────────────────────────────┤
│                   Storage Layer                          │
│   ┌───────────┐   ┌───────────┐   ┌───────────┐         │
│   │  SQLite   │   │ PostgreSQL│   │   Redis   │         │
│   └───────────┘   └───────────┘   └───────────┘         │
└──────────────────────────────────────────────────────────┘
```

## Data Model

Single `nodes` table. Root nodes (parent_id = NULL) carry conversation metadata.

```sql
CREATE TABLE nodes (
    id TEXT PRIMARY KEY,
    parent_id TEXT REFERENCES nodes(id),
    sequence INTEGER NOT NULL,
    node_type TEXT NOT NULL,           -- "user", "assistant", "system", "tool_call", "tool_result"
    content TEXT NOT NULL DEFAULT '',

    -- LLM execution metadata (on assistant nodes)
    model TEXT,
    tokens_in INTEGER,
    tokens_out INTEGER,
    latency_ms INTEGER,
    status TEXT,

    -- Root node metadata (NULL on non-root nodes)
    title TEXT,
    system_prompt TEXT,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_nodes_parent ON nodes(parent_id);
CREATE INDEX idx_nodes_root ON nodes(parent_id) WHERE parent_id IS NULL;
```

## REST API

Start the server:

```bash
langdag serve --port 8080
```

### Endpoints

```
POST   /prompt                     Start new conversation tree
POST   /nodes/{id}/prompt          Continue from existing node
GET    /nodes                      List root nodes
GET    /nodes/{id}                 Get a single node
GET    /nodes/{id}/tree            Get full tree from node
DELETE /nodes/{id}                 Delete node and subtree
GET    /health                     Health check
GET    /workflows                  List workflows
POST   /workflows                  Create workflow
POST   /workflows/{id}/run         Run workflow
```

### Prompt Request

`POST /prompt` and `POST /nodes/{id}/prompt` accept:

```json
{
    "message": "string",
    "model": "string (optional)",
    "system_prompt": "string (optional, only for /prompt)",
    "stream": false
}
```

### SSE Event Types

When `stream: true`, responses are sent as Server-Sent Events:

- `node_info` — Node metadata (node_id, node_type)
- `delta` — Streaming content token
- `done` — Stream complete
- `error` — Error occurred

## SDKs

### Python

```bash
pip install langdag
```

```python
from langdag import LangDAGClient

with LangDAGClient() as client:
    # Start a conversation
    node = client.prompt("What is LangDAG?")
    print(node.content)

    # Continue from any node
    node2 = node.prompt("Tell me more")

    # Branch from earlier node
    alt = node.prompt("Different angle")

    # Stream from a node
    for event in node.prompt_stream("Explain in detail"):
        if event.content:
            print(event.content, end="")

    # List root nodes and explore trees
    roots = client.list_roots()
    tree = client.get_tree(node.id)
```

### Go

```bash
go get github.com/langdag/langdag-go
```

```go
client := langdag.NewClient("http://localhost:8080")

// Start a conversation
node, err := client.Prompt(ctx, "What is LangDAG?",
    langdag.WithSystem("You are helpful."))
fmt.Println(node.Content)

// Continue from any node
node2, err := node.Prompt(ctx, "Tell me more")

// Branch from earlier node
alt, err := node.Prompt(ctx, "Different angle")

// Stream
stream, err := client.PromptStream(ctx, "Explain graphs")
for event := range stream.Events() {
    fmt.Print(event.Content)
}
result, err := stream.Node()

// List root nodes and explore trees
roots, err := client.ListRoots(ctx)
tree, err := client.GetTree(ctx, nodeID)
```

### TypeScript

```bash
npm install langdag
```

```typescript
import { LangDAGClient } from 'langdag';

const client = new LangDAGClient();

// Start a conversation
const node = await client.prompt("What is LangDAG?");
console.log(node.content);

// Continue from any node
const node2 = await node.prompt("Tell me more");

// Branch from earlier node
const alt = await node.prompt("Different angle");

// Stream
const stream = await client.promptStream("Explain graphs");
for await (const event of stream.events()) {
  process.stdout.write(event.content);
}
const result = await stream.node();

// List root nodes and explore trees
const roots = await client.listRoots();
const tree = await client.getTree(nodeId);
```

## CLI Usage

```bash
# Set your API key
export ANTHROPIC_API_KEY="your-api-key"

# Start a new conversation
langdag prompt "What is LangDAG?"

# Continue from a node
langdag prompt <node-id> "Tell me more"

# Interactive mode
langdag prompt
langdag prompt <node-id>

# Flags
langdag prompt -m <model> "message"
langdag prompt -s "system prompt" "message"

# Node management
langdag ls                              # List root nodes
langdag show <id>                       # Show node tree
langdag rm <id>                         # Delete node + subtree
```

## Workflow YAML Format

```yaml
name: research_agent
description: Research a topic

defaults:
  model: claude-sonnet-4-20250514
  max_tokens: 4096

nodes:
  - id: input
    type: input
  - id: researcher
    type: llm
    system: "You are a research assistant."
    prompt: "Research this topic: {{input}}"
  - id: output
    type: output

edges:
  - from: input
    to: researcher
  - from: researcher
    to: output
```

## Configuration

Environment variables (prefixed with `LANGDAG_`):

- `LANGDAG_PROVIDER` — LLM provider (e.g., `anthropic`, `mock`)
- `ANTHROPIC_API_KEY` — API key for Anthropic provider

## Links

- Website: https://langdag.com
- GitHub: https://github.com/aduermael/langdag
- License: MIT
